{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import torch\n",
    "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de', cache_dir='.embeddings')\n",
    "tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de', cache_dir='.embeddings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  38,  121,  751,   12,   12, 6076,  166,   54, 4843,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  105, 29049,    30,    57,  3726,    96, 31597,     0]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = ['I am going to tokyo by bus']\n",
    "tgt_text = ['Ich fahre mit dem Bus nach Tokio']\n",
    "tokenized = tokenizer.prepare_seq2seq_batch(src_text, tgt_text)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def shift_tokens_right(input_ids, pad_token_id):\n",
    "    \"\"\"Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\"\"\"\n",
    "    prev_output_tokens = input_ids.clone()\n",
    "    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
    "    prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
    "    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
    "    return prev_output_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ich fahre mit dem Bus nach Tokyo.']\n",
      "['Ich werde go tokykyo mit ']\n",
      "['Bus To']\n",
      "['Bus To']\n",
      "['Ich fahre mit dem Bus nach Tokio']\n"
     ]
    }
   ],
   "source": [
    "translated = model.generate(tokenized['input_ids'], temperature=0.8)\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "print(tgt_text)\n",
    "\n",
    "\n",
    "outputs = model(tokenized['input_ids'])\n",
    "logits = outputs[0]\n",
    "tgt_text2 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text2)\n",
    "\n",
    "\n",
    "outputs = model(tokenized['input_ids'], decoder_input_ids=tokenized['labels'], use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text3 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text3)\n",
    "\n",
    "\n",
    "outputs = model(tokenized['input_ids'], decoder_input_ids=tokenized['labels'][:, :-1].contiguous(), use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text4 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text4)\n",
    "\n",
    "\n",
    "decoder_input_ids = shift_tokens_right(tokenized['labels'], tokenizer.pad_token_id)\n",
    "outputs = model(tokenized['input_ids'], decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text5 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-base', cache_dir='.embeddings')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', cache_dir='.embeddings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "src_text = ['who <mask> 8 star rating with over 8 reviews in \" fonte \" ?']\n",
    "tgt_text = ['who has a 8 star rating with over 8 reviews in \" fonte \" ?']\n",
    "encoded_input_ids = tokenizer.batch_encode_plus(src_text)['input_ids']\n",
    "encoded_input_attention_masks = tokenizer.batch_encode_plus(src_text)['attention_mask']\n",
    "encoded_output_ids = tokenizer.batch_encode_plus(tgt_text)['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  8155, 50264,   290,   999,   691,    19,    81,   290,  6173,\n",
      "            11,    22,   856, 13757,    22, 17487,     2]])\n",
      "tensor([[    0,  8155,    34,    10,   290,   999,   691,    19,    81,   290,\n",
      "          6173,    11,    22,   856, 13757,    22, 17487,     2]])\n"
     ]
    }
   ],
   "source": [
    "tokenized = {}\n",
    "tokenized['input_ids'] = torch.tensor(encoded_input_ids)\n",
    "tokenized['attention_mask'] = torch.tensor(encoded_input_attention_masks)\n",
    "tokenized['decoder_input_ids'] = torch.tensor(encoded_output_ids)\n",
    "print(tokenized['input_ids'])\n",
    "print(tokenized['decoder_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['who wants to have an 8 star rating with over 8 reviews in \" fonte \" ?']\n",
      "['who has 8 star rating with over 8 reviews in \" fonte \"?']\n",
      "['who a 8 star rating with over 8 reviews in \" fonte \"?']\n",
      "['who a 8 star rating with over 8 reviews in \" fonte \"?']\n",
      "tensor([[    0,  8155,    34,    10,   290,   999,   691,    19,    81,   290,\n",
      "          6173,    11,    22,   856, 13757,    22, 17487,     2]])\n",
      "tensor([[    2,     0,  8155,    34,    10,   290,   999,   691,    19,    81,\n",
      "           290,  6173,    11,    22,   856, 13757,    22, 17487]])\n",
      "['who has a 8 star rating with over 8 reviews in \" fonte \"?']\n"
     ]
    }
   ],
   "source": [
    "translated = model.generate(tokenized['input_ids'],\n",
    "                                 bad_words_ids=None,\n",
    "                                 attention_mask=tokenized['attention_mask'],\n",
    "                                 decoder_start_token_id=0,\n",
    "                                 min_length=5,\n",
    "                                 max_length=20,\n",
    "                                 num_beams=1,\n",
    "                                 top_k=0,\n",
    "                                 top_p=0.9,\n",
    "                                 early_stopping=True,\n",
    "                                 num_return_sequences=1,\n",
    "                                 repetition_penalty=1.0,\n",
    "                                 no_repeat_ngram_size=0,\n",
    "                                 do_sample=True,\n",
    "                                 temperature=1.0, # if temperature==0, we do not sample\n",
    "                                 use_cache=False)\n",
    "tgt_text = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=False) for t in translated]\n",
    "print(tgt_text)\n",
    "\n",
    "\n",
    "outputs = model(tokenized['input_ids'], use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text2 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text2)\n",
    "\n",
    "\n",
    "outputs = model(tokenized['input_ids'], decoder_input_ids=tokenized['decoder_input_ids'], use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text3 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text3)\n",
    "\n",
    "\n",
    "outputs = model(tokenized['input_ids'], decoder_input_ids=tokenized['decoder_input_ids'][:, :-1].contiguous(), use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text4 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text4)\n",
    "\n",
    "\n",
    "print(tokenized['decoder_input_ids'])\n",
    "decoder_input_ids = shift_tokens_right(tokenized['decoder_input_ids'], tokenizer.pad_token_id)\n",
    "print(decoder_input_ids)\n",
    "outputs = model(tokenized['input_ids'], decoder_input_ids=decoder_input_ids, use_cache=False)\n",
    "logits = outputs[0]\n",
    "tgt_text5 = [tokenizer.decode(t, skip_special_tokens=True) for t in logits.max(-1)[1]]\n",
    "print(tgt_text5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_types_0905 = ujson.load(open('./bootleg_material/emb_data/wikidata_types_0905.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_to_typeid_0905 = ujson.load(open('./bootleg_material/emb_data/wikidata_to_typeid_0905.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidatatitle_to_typeid_0905 = ujson.load(open('./bootleg_material/emb_data/wikidatatitle_to_typeid_0905.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bootlegTypeID_to_wikidataTypeQID = {}\n",
    "for k, v in wikidata_to_typeid_0905.items():\n",
    "    bootlegTypeID_to_wikidataTypeQID[v] = wikidatatitle_to_typeid_0905[k]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ujson.dump(bootlegTypeID_to_wikidataTypeQID, open('./bootleg_material/emb_data/bootlegTypeID_to_wikidataTypeQID.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidataTypeQID_to_bootlegTypeID = ujson.load(open('./bootleg_material/emb_data/wikidataqid_to_bootlegtypeid.json'))\n",
    "bootlegTypeID_to_wikidataTypeQID = {v:k for k,v in wikidataTypeQID_to_bootlegTypeID.items()}\n",
    "\n",
    "from collections import defaultdict\n",
    "entityQID_to_wikidataTypeQID = defaultdict(list)\n",
    "for k, v in wikidata_types_0905.items():\n",
    "    entityQID_to_wikidataTypeQID[k] = [bootlegTypeID_to_wikidataTypeQID[val] for val in v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ujson.dump(entityQID_to_wikidataTypeQID, open('./bootleg_material/emb_data/entityQID_to_wikidataTypeQID.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
